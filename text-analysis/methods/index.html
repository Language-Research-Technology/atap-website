<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Useful Methods - ATAP</title><meta name=Description content="The Australian Text Analytics Platform is an open source environment that provides researchers with tools and training for analysing, processing, and exploring text."><meta property="og:title" content="Useful Methods"><meta property="og:description" content="Counting words More complex methods - Classification More complex methods - Others Visualisation
Introduction Throughout this page, we have given links to further information in Wikipedia and in the tutorials provided by the Language Technology and Data Analysis Laboratory LADAL at the University of Queensland. We also have given references to published research using the methods we discuss.
LADAL has an overview of text analysis and distant reading.
Counting Words Word frequency Knowing how frequently words occur in a text can already give us information about that text and frequency lists based on large corpora are a useful tool in themselves - you can download such lists for the (original) British National Corpus."><meta property="og:type" content="article"><meta property="og:url" content="https://www.atap.edu.au/text-analysis/methods/"><meta property="og:image" content="https://www.atap.edu.au/atap-logo.png"><meta property="article:section" content="text-analysis"><meta property="article:published_time" content="2022-02-15T16:46:42+10:00"><meta property="article:modified_time" content="2022-02-15T16:46:42+10:00"><meta property="og:site_name" content="ATAP"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.atap.edu.au/atap-logo.png"><meta name=twitter:title content="Useful Methods"><meta name=twitter:description content="Counting words More complex methods - Classification More complex methods - Others Visualisation
Introduction Throughout this page, we have given links to further information in Wikipedia and in the tutorials provided by the Language Technology and Data Analysis Laboratory LADAL at the University of Queensland. We also have given references to published research using the methods we discuss.
LADAL has an overview of text analysis and distant reading.
Counting Words Word frequency Knowing how frequently words occur in a text can already give us information about that text and frequency lists based on large corpora are a useful tool in themselves - you can download such lists for the (original) British National Corpus."><meta name=application-name content="ATAP"><meta name=apple-mobile-web-app-title content="ATAP"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://www.atap.edu.au/text-analysis/methods/><link rel=next href=https://www.atap.edu.au/text-analysis/data-prep/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Useful Methods","inLanguage":"en-us","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/www.atap.edu.au\/text-analysis\/methods\/"},"genre":"text-analysis","wordcount":2361,"url":"https:\/\/www.atap.edu.au\/text-analysis\/methods\/","datePublished":"2022-02-15T16:46:42+10:00","dateModified":"2022-02-15T16:46:42+10:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Australian Text Analytics Project"},"description":""}</script></head><body data-header-desktop=normal data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=ATAP><img class="lazyload logo" src=/svg/loading.min.svg data-src=/atap-logo.png data-srcset="/atap-logo.png, /atap-logo.png 1.5x, /atap-logo.png 2x" data-sizes=auto alt=/atap-logo.png title=/atap-logo.png></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/>Home </a><a class=menu-item href=/text-analysis/overview/>Text Analysis </a><a class=menu-item href=/tools/>Tools </a><a class=menu-item href=/posts/>Blog </a><a class=menu-item href=/events/events>Events </a><a class=menu-item href=/resources/>Resources </a><a class=menu-item href=/organisation/>Organisation </a><a class=menu-item href=/contact/>News & Contact </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=ATAP><img class="lazyload logo" src=/svg/loading.min.svg data-src=/atap-logo.png data-srcset="/atap-logo.png, /atap-logo.png 1.5x, /atap-logo.png 2x" data-sizes=auto alt=/atap-logo.png title=/atap-logo.png></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/ title>Home</a><a class=menu-item href=/text-analysis/overview/ title>Text Analysis</a><a class=menu-item href=/tools/ title>Tools</a><a class=menu-item href=/posts/ title>Blog</a><a class=menu-item href=/events/events title>Events</a><a class=menu-item href=/resources/ title>Resources</a><a class=menu-item href=/organisation/ title>Organisation</a><a class=menu-item href=/contact/ title>News & Contact</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class="page single special"><h1 class="single-title animate__animated animate__pulse animate__faster">Useful Methods</h1><div class=content id=content><p><a href=#counting-words rel>Counting words</a>   
<a href=#classification rel>More complex methods - Classification</a>   
<a href=#others rel>More complex methods - Others</a>   
<a href=#visualisation rel>Visualisation</a></p><h3 id=introduction>Introduction</h3><p>Throughout this page, we have given links to further information in Wikipedia and in the tutorials provided by the Language Technology and Data Analysis Laboratory <a href=https://slcladal.github.io/ target=_blank rel="noopener noreffer">LADAL</a> at the University of Queensland. We also have given references to published research using the methods we discuss.</p><p>LADAL has an overview of <a href=https://slcladal.github.io/textanalysis.html target=_blank rel="noopener noreffer">text analysis and distant reading</a>.</p><h3 id=counting-words>Counting Words</h3><h4 id=word-frequency>Word frequency</h4><p>Knowing how frequently words occur in a text can already give us information about that text and frequency lists based on large corpora are a useful tool in themselves - you can <a href=https://kilgarriff.co.uk/bnc-readme.html target=_blank rel="noopener noreffer">download</a> such lists for the (original) <a href=http://www.natcorp.ox.ac.uk/ target=_blank rel="noopener noreffer">British National Corpus</a>.</p><p>Tracking changes in the frequency of use of words across time has become popular since Google’s <a href=https://books.google.com/ngrams target=_blank rel="noopener noreffer">n-gram viewer</a> has been available. However, results from this tool have to treated with caution for reasons set out in this <a href=https://broadstreet.blog/2021/08/11/bad-ngrams/ target=_blank rel="noopener noreffer">blog-post</a>.</p><p>Comparing patterns of word frequency across texts can be part of authorship attribution. Patrick Juola <a href="https://languagelog.ldc.upenn.edu/nll/?p=5315" target=_blank rel="noopener noreffer">describes using this method</a> when he tried to decide whether Robert Galbraith was really J.K Rowling.</p><p>This paper uses frequency and concordance analysis, with Australian data:
<data id=id-1 data-raw></data>
The ratio of types and tokens in a text has been used as a measure of lexical diversity in developmental and clinical studies as well as in stylistics. It has also been applied to theoretical problems in linguistics:
<data id=id-2 data-raw></data></p><h4 id=concordance>Concordance</h4><p>A concordance allows the researcher to see all instances of a word or phrase in a text, neatly aligned in a column and with preceding and following context (see image below).
Concordances are often a first step in analysis. The concordance allows a researcher to see how a word is used and in what contexts. Most concordancing tools allow sorting of results by either preceding or following words – the coloured text in the example below shows that in this case the results have been sorted hierarchically on the three following words. This possibility can help in discovering patterns of co-occurrence. Concordances are also very useful when looking for good examples to illustrate a point. (The type of display seen in the example is often referred to as KeyWord In Context – KWIC. There is a possibility of confusion here, as there is a separate analytic method commonly referred to as <a href=#keywords rel>Keywords</a>.)</p><data id=id-3 data-raw></data><p>This <a href=https://slcladal.github.io/kwics.html target=_blank rel="noopener noreffer">tutorial</a> from LADAL on concordancing uses a notebook containing R code as a method of extracting concordance data.</p><h4 id=clusters-and-collocations>Clusters and collocations</h4><p>Two methods can be used for counting the co-occurrence of items in text.
Clusters (sometimes known as n-grams) are sequences of adjacent items. A bigram is a sequence of two items, a trigram (3-gram) is a sequence of three items and so on. n-grams are types made up of more than one item, and therefore we can count the number of tokens of each n-gram in texts. n-grams are also the basis for a class of language models. (Google created a very large data set of English n-grams in developing their language-based algorithms and this <a href=https://storage.googleapis.com/books/ngrams/books/datasetsv3.html target=_blank rel="noopener noreffer">data is available</a>.)
Collocations are patterns of co-occurrence in which the items are not necessarily adjacent. An example of why this is important is verbs and their objects in English. The object of a verb is a noun phrase and in many cases the first item in an English noun phrase is a determiner. This means that for many transitive verbs, the bigram <em>verb the</em> will occur quite frequently. But it is much more interesting to know whether there are patterns relating verbs and the entities which are their objects.
Collocation analysis uncovers such patterns by looking at co-occurrences within a window of a certain size, for example three tokens on either side of the target. Collocation analysis gives information about the frequency of the co-occurrence of words and also a statistical measure of how likely that frequency is, given the overall frequencies of the terms in the corpus. Measures commonly applied include <a href=https://en.wikipedia.org/wiki/Mutual_information target=_blank rel="noopener noreffer">Mutual Information</a> scores and <a href=https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood target=_blank rel="noopener noreffer">Log-Likelihood</a> scores.
Collocations can also tell us about the meanings of words. If a word has collocates which fall into semantically distinct groups, this can indicate ambiguity or polysemy. And if different words share patterns of collocation, this can be evidence that the words are at least partial synonyms.</p><p>This graphic shows collocation relations in Darwin’s Origin of Species visualised as a network - the likelihood of a pair of words occurring in close proximity in the text is indicated by the weight of the line linking them:</p><p><data id=id-4 data-raw></data>
This article uses bigram frequencies as part of an analysis of language change:</p><p><data id=id-5 data-raw></data>
This research uses the discovery of shared patterns of collocation as evidence that the words are at least partial synonyms:
<data id=id-6 data-raw></data>
This <a href=https://slcladal.github.io/coll.html target=_blank rel="noopener noreffer">tutorial</a> from LADAL on analysing co-occurences and collocations uses a notebook containing R code as a method to extract and visualise semantic links between words.</p><h4 id=keywords>Keywords</h4><p>Keyword analysis is a statistically robust method of comparing frequencies of words in corpora. It tells us which words are more frequent (or less frequent) than would be expected in one text compared to another text and gives an estimate of the probability of the result. Keyword analysis uses two corpora: a <strong>target</strong> corpus, which is the material of interest, and a <strong>comparison</strong> corpus. Frequency lists are made for each corpus and then frequency of individual types in each corpus are compared. Keywords are ones which occur more ( or less) frequently in the target corpus than expected given the reference corpus.
The <strong>keyness</strong> of individual items is a quantitative measure of how unexpected the frequency is; chi-square is a one possible measure of this, but a log-likelihood measure is more commonly used. Positive keywords are words which occur more commonly than expected; negative keywords are words which occur less commonly than expected.</p><p>This visualisation shows a comparison of positive distinguishing words for three texts (Charles Darwin’s <em>Origin</em>, Herman Melville’s <em>Moby Dick</em>, and George Orwell’s <em>1984</em>), words that occur more commonly than we expect in one text when taking the other two texts as a comparison:</p><data id=id-7 data-raw></data><p>This paper applies keyword analysis to Australian text data sourced from a television series script:
<data id=id-8 data-raw></data>
Tony McEnery describes using the keyword analysis method to compare four varieties of English in this chapter:
<data id=id-9 data-raw></data>
This article explores how to assess Shakespeare’s use of words to build characters by using keyword analysis of the characters&rsquo; dialog:
<data id=id-10 data-raw></data></p><h3 id=classification>More complex methods – Classification</h3><p>Classification methods aim to assign some unit of analysis, such as a word or a document, to a class. For example, a document (or a portion of a document) can be classified as having positive or negative sentiment. These methods are all examples of supervised machine learning. An algorithm is trained on the basis of annotated data to identify classifiers in the data – features which correlate in some way with the annotated classifications. If the algorithm achieves good results on testing data (classified by human judgment), then it can be used to classify unannotated data.</p><h4 id=document-classification>Document Classification</h4><p>The task here is to assign documents to categories automatically. An everyday example of this procedure is spam filtering of email that may be applied by internet service providers and also within email applications. An example of this technique being used in research would be automatically identifying historical court records as referring either to violent crimes, property offences, or other crimes.</p><p>The following two articles taken together give an account of a technique for partially automating the training phase of classification and then of how the classifiers allowed researchers to access new information in a large and complex text.
<data id=id-11 data-raw></data>
<a href=https://en.wikipedia.org/wiki/Document_classification target=_blank rel="noopener noreffer">Wikipedia</a></p><h4 id=sentiment-analysis>Sentiment analysis</h4><p>Sentiment analysis assigns documents according to the affect which they express. In simple cases, this can mean sorting documents into those which a express a positive view and those which express a negative view (with a neutral position sometimes also included). Such classifications are the basis for aggregated ratings - for example, online listings of movies and restaurants. A sentiment value is assigned to individual reviews then an aggregate score is calculated based on those values and that aggregate score is the rating presented to the user. More sophisticated sentiment analysis can assign values on a scale. Some sentiment analysis tools use dictionaries of terms with sentiment values assigned to those terms; these are known as pre-trained or pre-determined classifiers.</p><p>The following figure shows the results of the sentiment analysis of four texts (<em>The Adventures of Huckleberry Finn</em> by Mark Twain, <em>1984</em> by George Orwell, <em>The Colour out of Space</em> by H.P.Lovecraft, and <em>On the Origin of Species</em> by Charles Darwin) using the Word-Emotion Association Lexicon (Mohammad and Turney 2013). The graphic shows what percentage of each text can be assigned to each of eight categories of sentiment:</p><data id=id-12 data-raw></data><p>The Wikipedia entry for <a href=https://en.wikipedia.org/wiki/Sentiment_analysis target=_blank rel="noopener noreffer">Sentiment Analysis</a> gives more information and examples particularly in relation to the use of sentiment analysis as a tool used in online settings.</p><p>LADAL’s <a href=https://slcladal.github.io/sentiment.html target=_blank rel="noopener noreffer">Sentiment Analysis tutorial</a> uses a notebook containing R code as a method of performing sentiment analysis.</p><p>This article discusses problems in assembling training data for complex sentiment analysis tasks and then applies the results to oral history interviews with Holocaust survivors:
<data id=id-13 data-raw></data></p><h4 id=named-entity-recognition>Named Entity Recognition</h4><p>Named Entity Recognition involves two levels of classification. First, segments of text are classified as either denoting or not denoting an entity: for example, a person, a place or an organization. The identified entities can then be classified as belonging to one of the types of entity.</p><p>The Wikipedia entry explaining <a href=https://en.wikipedia.org/wiki/Named-entity_recognition target=_blank rel="noopener noreffer">named-entity recognition</a> gives further detail about the technique.</p><p>This article looks at the problems encountered when applying a well-known entity recognition package (<a href=https://nlp.stanford.edu/software/CRF-NER.html target=_blank rel="noopener noreffer">Stanford</a>) to historical newspapers in the National Library of Australia’s Trove collection:
<data id=id-14 data-raw></data>
This article (section 6.3) discusses why entity recognition is not as useful as might be expected when studying names in novels:
<data id=id-15 data-raw></data></p><h4 id=computational-stylistics-stylometry>Computational Stylistics (Stylometry)</h4><p>This method is also referred to as authorship attribution as the classification task is to assess patterns of language use in order to decide whether to attribute a piece of text to a particular author (and with what degree of confidence). Seemingly simple classifiers are used for this task as they are assumed to be less open to conscious manipulation by writers. For example, comparative patterns of occurrence of function words such as <em>the</em> and <em>a/an</em> are considered a better classifier than occurrences of content words. Character n-grams, that is sequences of characters of a specified length, have also proven to be a good classifier for use in this task. A recent example of these techniques being applied in a case which received a good deal of public attention was the controversy about whether <a href="https://languagelog.ldc.upenn.edu/nll/?p=5315" target=_blank rel="noopener noreffer">Robert Galbraith was really J.K Rowling</a>.</p><p>The Wikipedia entry on <a href=https://en.wikipedia.org/wiki/Stylometry target=_blank rel="noopener noreffer">stylometry</a> gives further information on the methodology.</p><p>This article applies stylometric techniques to a classic of Chinese literature:
<data id=id-16 data-raw></data></p><p>An overview of the use of function words in stylometry:
<data id=id-17 data-raw></data>
A classic stylometric study using Bayesian statistics rather than machine learning is:
<data id=id-18 data-raw></data></p><h3 id=others>More complex methods – Others</h3><h4 id=topic-models>Topic models</h4><p>Topic modeling is a method which tries to recover abstract ‘topics’ which occur in a collection of documents. The underlying assumption is that different topics will tend to be associated with different words, different documents will tend to be associated with different topics, and therefore the distribution of words across documents allows us to find topics. The complete model includes the strength of association (or probability) between each word and each topic, and between each topic and each document. A topic consists of a group of words and it is up to the researcher to decide if a semantically coherent interpretation can be given to any of the topics recovered. The number of topics to be recovered is specified in advance.</p><p>The example visualisation below is based on topic modeling of the State of the Union addresses given by US presidents, and shows the relative importance of different topics over time. In the right hand part of the figure, the words most closely linked to each topic are listed; the researcher has not attempted to give labels to these (although in some cases, it is not too hard to imagine what labels we might use). Note also that words are not uniquely linked to topics - for example, the word <em>state</em> is closely linked to seven of the topics in this model.</p><data id=id-19 data-raw></data><p>The Wikipedia entry for <a href=https://en.wikipedia.org/wiki/Topic_model target=_blank rel="noopener noreffer">topic models</a> gives a more detailed explanation of the process.</p><p>This <a href=https://slcladal.github.io/topicmodels.html target=_blank rel="noopener noreffer">topic modeling tutorial</a> from LADAL uses R coding to process textual data and generate a topic model from that data.</p><p>Poetics 41(6) is a journal issue devoted to the use of topic models in literary studies: the introduction to the journal (by Mohr and Bogdanov: <a href=https://doi.org/10.1016/j.poetic.2013.10.001 target=_blank rel="noopener noreffer">https://doi.org/10.1016/j.poetic.2013.10.001</a>) provides a useful overview of the method.</p><p>And this paper uses topic modeling as one tool in trying to improve access to a huge collection of scholarly literature:
<data id=id-20 data-raw></data></p><h4 id=network-analysis>Network Analysis</h4><p>Network analysis allows us to produce visualisations of the relationships between entities within a dataset. Analysis of social networks is a classic application of the method, but words and documents can also be thought of as entities and the relationships between them can then be analysed with this method. (see example visualisation of Darwin’s <em>Origin of Species</em> above)
Here is another example of a network graph illustrating the relationships between the characters of Shakespeare’s <em>Romeo and Juliet</em>:</p><data id=id-21 data-raw></data><p>This article gives several examples of how representing collocational links between words as a network can lead to insight into meaning relations:
<data id=id-22 data-raw></data>
Wikipedia has articles on network theory in <a href=https://en.wikipedia.org/wiki/Network_theory target=_blank rel="noopener noreffer">general</a> and on <a href=https://en.wikipedia.org/wiki/Social_network_analysis target=_blank rel="noopener noreffer">social network analysis</a>.
in particular.</p><p>LADAL’s tutorial on <a href=https://slcladal.github.io/net.html target=_blank rel="noopener noreffer">Network Analysis</a> introduces this method using R coding.</p><h3 id=visualisation>Visualisation</h3><p>Visualisation is an important technique for exploring data, allowing us to see patterns easily, and also for presenting results.
There are many methods for creating visualisations and this article gives an overview of some possibilities for visualising corpus data:
<data id=id-23 data-raw></data>
If you would like to see something more complex, this article includes animations showing change in use of semantic space over time – but you need to have full access to the online publication to see it.
<data id=id-24 data-raw></data>
This LADAL tutorial on <a href=https://slcladal.github.io/introviz.html target=_blank rel="noopener noreffer">data visualisation</a> in R makes use of the <a href=https://ggplot2.tidyverse.org/ target=_blank rel="noopener noreffer">ggplot2</a> package to create some common data visualisations using code.</p><p><a href=../overview rel>Overview</a>    <a href=../data-prep/ rel>Data Preparation</a>    <a href=../research-objects rel>Research Objects</a></p></div></div></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.111.3">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2021 - 2023</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>Australian Text Analytics Project</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by/4.0/ target=_blank>CC BY 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript src=https://platform.twitter.com/widgets.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},data:{"id-1":`

\u003cdiv class="reference"\u003eBednarek, Monika. 2020. Invisible or high-risk: Computer-assisted discourse analysis of references to Aboriginal and Torres Strait Islander people(s) and issues in a newspaper corpus about diabetes. \u003ci\u003ePLoS ONE\u003c/i\u003e 15/6: e0234486. \u003ca href="https://doi.org/10.1371/journal.pone.0234486" target="_blank"\u003ehttps://doi.org/10.1371/journal.pone.0234486\u003c/a\u003e \u003c/div\u003e\u003e
`,"id-10":`
\u003cdiv class="reference"\u003eCulpeper, Jonathan. 2002. Computers, language and characterisation: An analysis of six characters in Romeo and Juliet. In \u003ci\u003eConversation in Life and in Literature: Papers from the ASLA Symposium\u003c/i\u003e (Association Suedoise de Linguistique Appliquee (ASLA) 15), 11–30. Uppsala: Universitetstryckeriet. (\u003ca href="https://lexically.net/wordsmith/corpus_linguistics_links/Keywords-Culpeper.pdf" target="_blank"\u003epdf\u003c/a\u003e)\u003c/div\u003e
`,"id-11":`

\u003cdiv class="reference"\u003eLeavy, Susan, Mark T Keane \u0026 Emilie Pine. 2019. Patterns in language: Text analysis of government reports on the Irish industrial school system with word embedding. \u003ci\u003eDigital Scholarship in the Humanities\u003c/i\u003e 34(Supplement_1). i110–i122. \u003ca href="https://doi.org/10.1093/llc/fqz012" target="_blank"\u003ehttps://doi.org/10.1093/llc/fqz012\u003c/a\u003e.
\u003cbr /\u003e
Pine, Emilie, Susan Leavy \u0026 Mark T. Keane. 2017. Re-reading the Ryan Report: Witnessing via Close and Distant Reading. \u003ci\u003eÉire-Ireland\u003c/i\u003e 52(1–2). 198–215. https://doi.org/10.1353/eir.2017.0009. (\u003ca href="https://researchrepository.ucd.ie/handle/10197/10287" target="_blank"\u003eavailable online\u003c/a\u003e)\u003c/div\u003e
`,"id-12":`

\u003cimg src="sentiment_analysis.png" title="Sentiment analysis of four texts" height="500" class="center_image" /\u003e

\u003cdiv style="text-align: center;"\u003e\u003ch4\u003eSentiment analysis of four texts\u003c/h4\u003e\u003c/div\u003e
\u003cbr /\u003e
`,"id-13":`

\u003cdiv class="reference"\u003eBlanke, Tobias, Michael Bryant \u0026 Mark Hedges. 2020. Understanding memories of the Holocaust—A new approach to neural networks in the digital humanities. \u003ci\u003eDigital Scholarship in the Humanities\u003c/i\u003e 35(1). 17–33. \u003ca href="https://doi.org/10.1093/llc/fqy082" target="_blank"\u003ehttps://doi.org/10.1093/llc/fqy082\u003c/a\u003e.\u003c/div\u003e
`,"id-14":`

\u003cdiv class="reference"\u003eMac Kim, Sunghwan \u0026 Steve Cassidy. 2015. Finding names in Trove: Named Entity Recognition for Australian historical newspapers. In \u003ci\u003eProceedings of the Australasian Language Technology Association Workshop 2015\u003c/i\u003e, 57–65. (\u003ca href="https://aclanthology.org/U15-1007.pdf" target="_blank"\u003epdf\u003c/a\u003e)\u003c/div\u003e
`,"id-15":`
\u003cdiv class="reference"\u003eDalen-Oskam, K. van. 2013. Names in novels: An experiment in computational stylistics. \u003ci\u003eLiterary and Linguistic Computing\u003c/i\u003e 28(2). 359–370. \u003ca href="https://doi.org/10.1093/llc/fqs007" target="_blank"\u003ehttps://doi.org/10.1093/llc/fqs007\u003c/a\u003e.\u003c/div\u003e
`,"id-16":`

\u003cdiv class="reference"\u003eZhu, Haoran, Lei Lei \u0026 Hugh Craig. 2021. Prose, Verse and Authorship in Dream of the Red Chamber: A Stylometric Analysis. \u003ci\u003eJournal of Quantitative Linguistics\u003c/i\u003e 28(4). 289–305. \u003ca href="https://doi.org/10.1080/09296174.2020.1724677" target="_blank"\u003ehttps://doi.org/10.1080/09296174.2020.1724677\u003c/a\u003e.\u003c/div\u003e
`,"id-17":`

\u003cdiv class="reference"\u003eGarcia, A. M. \u0026 J. C. Martin. 2007. Function Words in Authorship Attribution Studies. \u003ci\u003eLiterary and Linguistic Computing\u003c/i\u003e 22(1). 49–66. \u003ca href="https://doi.org/10.1093/llc/fql048" target="_blank"\u003ehttps://doi.org/10.1093/llc/fql048\u003c/a\u003e.\u003c/div\u003e
`,"id-18":`
\u003cdiv class="reference"\u003eMosteller, Frederick \u0026 David Lee Wallace. 1984. \u003ci\u003eApplied Bayesian and classical inference: the case of the Federalist papers\u003c/i\u003e. New York: Springer-Verlag.\u003c/div\u003e
`,"id-19":`

\u003cimg src="topic_models.png" title="Topics in the State of the Union Address over time" height="500" class="center_image" /\u003e

\u003cdiv style="text-align: center;"\u003e\u003ch4\u003eTopics in the State of the Union Address over time\u003c/h4\u003e\u003c/div\u003e
\u003cbr /\u003e
`,"id-2":`
\u003cdiv class="reference"\u003eKettunen, Kimmo. 2014. Can Type-Token Ratio be Used to Show Morphological Complexity of Languages? \u003ci\u003eJournal of Quantitative Linguistics\u003c/i\u003e 21(3). 223–245. \u003ca href="https://doi.org/10.1080/09296174.2014.911506" target="_blank"\u003ehttps://doi.org/10.1080/09296174.2014.911506\u003c/a\u003e.\u003c/div\u003e
`,"id-20":`

\u003cdiv class="reference"\u003eMimno, David. 2012. Computational historiography: Data mining in a century of classics journals. \u003ci\u003eJournal on Computing and Cultural Heritage\u003c/i\u003e 5(1). 1–19. \u003ca href="https://doi.org/10.1145/2160165.2160168" target="_blank"\u003ehttps://doi.org/10.1145/2160165.2160168\u003c/a\u003e.\u003c/div\u003e
`,"id-21":`

\u003cimg src="network_RandJ-1.png" title="Network of characters in Romeo and Juliet" height="400" class="center_image" /\u003e

\u003cdiv style="text-align: center;"\u003e\u003ch4\u003eNetwork of characters in \u003ci\u003eRomeo and Juliet\u003c/i\u003e\u003c/h4\u003e\u003c/div\u003e
\u003cbr /\u003e
`,"id-22":`

\u003cdiv class="reference"\u003eBrezina, Vaclav, Tony McEnery \u0026 Stephen Wattam. 2015. Collocations in context: A new perspective on collocation networks. \u003ci\u003eInternational Journal of Corpus Linguistics\u003c/i\u003e 20(2). 139–173. \u003ca href="https://doi.org/10.1075/ijcl.20.2.01bre" target="_blank"\u003ehttps://doi.org/10.1075/ijcl.20.2.01bre\u003c/a\u003e. (pdf)\u003c/div\u003e
`,"id-23":`

\u003cdiv class="reference"\u003eSiirtola, Harri, Terttu Nevalainen, Tanja Säily \u0026 Kari-Jouko Räihä. 2011. Visualisation of text corpora: A case study of the PCEEC. \u003ci\u003eHow to Deal with Data: Problems and Approaches to the Investigation of the English Language over Time and Space\u003c/i\u003e. Helsinki: VARIENG 7. \u003ca href="https://varieng.helsinki.fi/series/volumes/07/siirtola_et_al/index.html" target="_blank"\u003e[html]\u003c/a\u003e\u003c/div\u003e
`,"id-24":`
\u003cdiv class="reference"\u003eHilpert, Martin \u0026 Florent Perek. 2015. Meaning change in a petri dish: constructions, semantic vector spaces, and motion charts. \u003ci\u003eLinguistics Vanguard\u003c/i\u003e 1(1). \u003ca href="https://doi.org/10.1515/lingvan-2015-0013" target="_blank"\u003ehttps://doi.org/10.1515/lingvan-2015-0013\u003c/a\u003e.\u003c/div\u003e
`,"id-3":`

\u003cimg src="concordance.png" title="Example of a concordance" height="400" class="center_image" /\u003e

\u003cdiv style="text-align: center;"\u003e\u003ch4\u003eExample of a concordance\u003c/h4\u003e
(The example here was produced by \u003ca href="https://www.laurenceanthony.net/software/antconc/" target="_blank"\u003eAntconc\u003c/a\u003e)\u003c/div\u003e
\u003cbr /\u003e
`,"id-4":`

\u003cimg src="collocation_network.png" title="Collocation patterns in Origin of Species as a network" height="400" class="center_image" /\u003e

\u003cdiv style="text-align: center;"\u003e\u003ch4\u003eCollocation patterns in \u003ci\u003eOrigin of Species\u003c/i\u003e as a network\u003c/h4\u003e\u003c/div\u003e
\u003cbr /\u003e
`,"id-5":`

\u003cdiv class="reference"\u003eSchweinberger, Martin. 2021. Ongoing change in the Australian English amplifier system. \u003ci\u003eAustralian Journal of Linguistics\u003c/i\u003e 41(2). 166–194. \u003ca href="https://doi.org/10.1080/07268602.2021.1931028" target="_blank"\u003ehttps://doi.org/10.1080/07268602.2021.1931028\u003c/a\u003e.\u003c/div\u003e
An article which uses concordances and collocation analysis:
\u003cdiv class="reference"\u003eBaker, Paul \u0026 Tony McEnery. 2005. A corpus-based approach to discourses of refugees and asylum seekers in UN and newspaper texts. \u003ci\u003eJournal of Language and Politics\u003c/i\u003e 4(2). 197–226.\u003c/div\u003e
`,"id-6":`
\u003cdiv class="reference"\u003eMcEnery, Tony \u0026 Helen Baker. 2017. \u003ci\u003eCorpus linguistics and 17th-century prostitution: computational linguistics and history\u003c/i\u003e (Corpus and Discourse. Research in Corpus and Discourse). London; New York, NY: Bloomsbury Academic. (especially chapters 4 and 5)\u003c/div\u003e
`,"id-7":`

\u003cimg src="keywords.png" title="Keywords from three texts" height="400" class="center_image" /\u003e

\u003cdiv style="text-align: center;"\u003e\u003ch4\u003eKeywords from three texts\u003c/h4\u003e\u003c/div\u003e
\u003cbr /\u003e
`,"id-8":`

\u003cdiv class="reference"\u003eBednarek, Monika. 2020. Keyword analysis and the indexing of Aboriginal and Torres Strait Islander identity: A corpus linguistic analysis of the Australian Indigenous TV drama Redfern Now. \u003ci\u003eInternational Journal of Corpus Linguistics\u003c/i\u003e 25/4: 369-99. \u003ca href="http://doi.org/10.1075/ijcl.00031.bed" target="_blank"\u003ehttp://doi.org/10.1075/ijcl.00031.bed\u003c/a\u003e\u003c/div\u003e
`,"id-9":`
\u003cdiv class="reference"\u003eMcEnery, Tony. 2016. Keywords. In Paul Baker \u0026 Jesse Egbert (eds.), *Triangulating methodological approaches in corpus-linguistic research* (Routledge Advances in Corpus Linguistics 17), 20–32. New York: Routledge.\u003c/div\u003e
`},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>