[{"categories":null,"content":" The Braided Channels research collection includes materials collected on Australia women, land and history in the Channel country. The collection is constructed from some 70 hours of oral history interviews with women from Australia’s Channel Country, together with archival film, transcripts, photos and music. It includes both audiovisual recordings and transcripts of interviews. See more The La Trobe Corpus of Spoken Australian English (LTCSAusE) comprises a collection of six recordings and transcriptions of spoken interaction amongst Australian speakers of English (some in conversation with native French speakers speaking English) made in Melbourne from 2001 to 2002. The recordings are transcribed using a combination of the Santa Barbara and conversation analysis transcription conventions. See more The contribution from AustLit provides full-text access to select samples of out of copyright poetry, fiction and criticism ranging from 1795 to the 1930s. The collection includes literature intended for popular audiences as well as literature intended for audiences concerned with literary quality or the establishment of a national canon. The bibliographical information associated with these records enables researchers to investigate the relationships between texts and particular publishers or to track the first publication of each text in newspapers, magazines or journals. The collection includes bibliographic and production information for works of fiction and poetry, writing for the theatre, biographical and travel writing, writing for film and television, criticism and reviews and the people and organisations who produce them. See more ","date":"2023-04-04","objectID":"/icons/icons-1/:0:0","tags":null,"title":"Icons - version 1","uri":"/icons/icons-1/"},{"categories":null,"content":"Braided Channels See more ","date":"2023-04-04","objectID":"/icons/icons-2/:1:0","tags":null,"title":"Icons - version 2","uri":"/icons/icons-2/"},{"categories":null,"content":"The La Trobe Corpus of Spoken Australian English See more ","date":"2023-04-04","objectID":"/icons/icons-2/:2:0","tags":null,"title":"Icons - version 2","uri":"/icons/icons-2/"},{"categories":null,"content":"AustLit See more ","date":"2023-04-04","objectID":"/icons/icons-2/:3:0","tags":null,"title":"Icons - version 2","uri":"/icons/icons-2/"},{"categories":null,"content":"The ATAP Quotation Tool is a Jupyter notebook containing code that was adapted and developed (with permission) from the GenderGapTracker by the Sydney Informatics Hub (SIH) in collaboration with the Sydney Corpus Lab as part of the Australian Text Analytics Platform (ATAP) project. The quotation tool is designed to identify and extract quoted content from newspaper texts. Since the tool uses a combination of syntactic and heuristic rules to extract quotes, it is able to identify quotes whether they are marked by quoting or projecting verbs (e.g. said) or not. The tool can also use Named-Entity Recognition to identify and classify the sources of these quotes and the entities within these quotes (as people, organisations, etc.). This is useful for answering a number of questions such as those related to representation of voices/sources (e.g. Who is cited the most/least? Who is not cited at all?) and those related to quoted content (e.g. What kind of information is sourced from others?). The tool can also identify the verbs that are used to cite quotations (e.g. say, tell, add, claim, admit). This will be particularly useful to those interested in the variation of reporting verbs. Once the tool has processed the files, it will display the first few identified and extracted quotes (and entities) in a table. An example of this preview table is shown in Table 1 below, which displays the identified quote along with information such as the speaker and their entity type, the entity name and type of the entities identified within the quote and the quoting/projecting verb (if there is one). The type of quote is described on the basis of the various components of the quote construction (Q = quotation mark, S = speaker, V = verb, C = content) and their linear order. The tool allows you to save and download the complete table of results as an Excel worksheet (.xslx format) for further analysis. It is also possible to preview all the identified quotes and entities in individual files. In this preview (see Figure 1 below), identified quotes and entities are presented in bold face and labelled accordingly (e.g. as quote/speaker or as a specific entity type such as PERSON, NORP, GPE – see legend to Figure 2 for abbreviations). You can download the visualisation such as the one shown in Figure 1 as an html file. The tool also allows you to visualise the top named entities identified in the quotes and/or the top entity types among identified speakers as bar graphs – an example is shown in Figure 2. You can choose to visualise the top entities for the whole corpus/dataset or individual files within the corpus/dataset. Other options for the visualisation include whether to display the entity names and/or types, and the number of top entities to display (i.e. in multiples of five). You can also choose to save the graphs (based on the set parameters) as jpg files. The tool is available on GitHub where you can launch the tool on Jupyter Notebook via Binder. That instance of Binder uses CILogon authentication, and you can access it by signing in with your (Australian) institutional login credentials or Google/Microsoft/Outlook account. If you have access to software that supports Jupyter Notebooks, you can also download the notebook to use locally (i.e. without Internet connection) on your own computer. If you have any questions, feedback, and/or comments about the tool, you can contact the SIH at sih.info@sydney.edu.au. ","date":"2023-03-28","objectID":"/posts/quotation-tool/:0:0","tags":null,"title":"Introducing the ATAP Quotation Tool: Extracting quotes and entities from newspaper articles\n","uri":"/posts/quotation-tool/"},{"categories":null,"content":"Acknowledgments This Jupyter notebook and relevant python scripts were developed by the Sydney Informatics Hub (SIH) in collaboration with the Sydney Corpus Lab under the Australian Text Analytics Platform program and the HASS Research Data Commons and Indigenous Research Capability Program. These projects received investment from the Australian Research Data Commons (ARDC), which is funded by the National Collaborative Research Infrastructure Strategy (NCRIS). ","date":"2023-03-28","objectID":"/posts/quotation-tool/:0:1","tags":null,"title":"Introducing the ATAP Quotation Tool: Extracting quotes and entities from newspaper articles\n","uri":"/posts/quotation-tool/"},{"categories":null,"content":"How to cite the notebook: If you are using this notebook in your research, please include the following statement or an appropriate variation thereof: This study has utilised a notebook/notebooks developed for the Australian Text Analytics Platform (https://www.atap.edu.au) available at https://github.com/Australian-Text-Analytics-Platform/quotation-tool. In addition, please inform ATAP (info@atap.edu.au) of publications and grant applications deriving from the use of any ATAP notebooks in order to support continued funding and development of the platform. ","date":"2023-03-28","objectID":"/posts/quotation-tool/:0:2","tags":null,"title":"Introducing the ATAP Quotation Tool: Extracting quotes and entities from newspaper articles\n","uri":"/posts/quotation-tool/"},{"categories":null,"content":"The Document Similarity tool is a Jupyter notebook containing code that was developed by the Sydney Informatics Hub (SIH) in collaboration with the Sydney Corpus Lab as part of the Australian Text Analytics Platform (ATAP) project. It uses MinHash to estimate the Jaccard similarity between sets of documents. This tool is helpful in cleaning data files in a dataset or corpus. This is particularly useful to those who collect media texts (e.g. of newspaper articles, tweets) where fully or partially duplicated texts are common occurrences. The tool is designed to compare every pair of texts in a dataset or corpus and produce similarity scores. Based on a similarity cut-off that has been specified (either the default cut-off parameters or ones that you set yourself), the tool retrieves pairs of texts where the similarity score exceeds the pre-determined cut-off. The result of this analysis is presented as a table containing a list of similar documents (in pairs) found by the tool – an example is shown in Table 1 below. Based on this analysis, the tool makes recommendations about which text within each similar pair should be kept or removed – as shown in the ‘status1’ and ‘status2’ columns in Table 1. The tool allows you to view each pair of similar documents (by specifying the row index you wish to analyse; see Figure 1 below), analyse them, and update the action/recommendation (i.e. ‘keep’ and ‘remove’). You can then download the non-duplicated texts (those labelled as keep) or the duplicated ones (those labelled as remove) into a zip archive of text (.txt) files. Additionally, the tool allows you to visualise the Jaccard similarity scores as a histogram (Figure 2). The histogram shows the Jaccard similarity scores for every pair of texts/documents in the corpus, and how many similar documents are found at those Jaccard similarity score ranges across the corpus. This is useful for estimating the extent of duplicated content in a corpus. Another, optional, visualisation shows the Jaccard similarity scores between specific pairs of texts/documents as a heatmap. This can be useful for identifying the texts that are most similar to each other, but works best with small numbers of texts. This optional feature of the tool could be used to compare the texts produced by different speakers/authors in a dataset. The tool is available on GitHub where you can launch the tool on Jupyter Notebook via Binder. That instance of Binder uses CILogon authentication, and you can access it by signing in with your (Australian) institutional login credentials or Google/Microsoft/Outlook account. If you have access to software that supports Jupyter Notebooks, you can also download the notebook to use locally (i.e. without Internet connection) on your own computer. If you have any questions, feedback, and/or comments about the tool, you can contact the SIH at sih.info@sydney.edu.au. ","date":"2023-03-07","objectID":"/posts/similarity-tool/:0:0","tags":null,"title":"Introducing the Document Similarity Tool: A new tool for corpus building\n","uri":"/posts/similarity-tool/"},{"categories":null,"content":"Acknowledgments This Jupyter notebook and relevant python scripts were developed by the Sydney Informatics Hub (SIH) in collaboration with the Sydney Corpus Lab under the Australian Text Analytics Platform program and the HASS Research Data Commons and Indigenous Research Capability Program. These projects received investment from the Australian Research Data Commons (ARDC), which is funded by the National Collaborative Research Infrastructure Strategy (NCRIS). The notebook incorporates MinHash, which is introduced by Andrei Z. Broder in this paper. Details can be found here. ","date":"2023-03-07","objectID":"/posts/similarity-tool/:0:1","tags":null,"title":"Introducing the Document Similarity Tool: A new tool for corpus building\n","uri":"/posts/similarity-tool/"},{"categories":null,"content":"How to cite the notebook: If you are using this notebook in your research, please include the following statement or an appropriate variation thereof: This study has utilised a notebook/notebooks developed for the Australian Text Analytics Platform (https://www.atap.edu.au) available at https://github.com/Australian-Text-Analytics-Platform/document-similarity/. In addition, please inform ATAP (info@atap.edu.au) of publications and grant applications deriving from the use of any ATAP notebooks in order to support continued funding and development of the platform. ","date":"2023-03-07","objectID":"/posts/similarity-tool/:0:2","tags":null,"title":"Introducing the Document Similarity Tool: A new tool for corpus building\n","uri":"/posts/similarity-tool/"},{"categories":null,"content":"Discursis is communication analytics technology that allows a user to analyse text based communication data, such as conversations, web forums and training scenarios. It uses natural language processing (NLP) algorithms to automatically process transcribed text to highlight participant interactions around specific topics and over the time-course of the conversation. Discursis can assist practitioners in understanding the structure, information content, and inter-speaker relationships that are present within input data. Discursis also provides quantitative measures of key metrics, such as topic introduction, topic consistency, and topic novelty. The NLP algorithms are used to construct a matrix of concept similarity scores between the sections into which a text has been divided. In the typical use case for this tool, that of a discourse with several speakers, those sections will be speaker turns and the similarity matrix provides information about the extent to which any pair of turns share concepts. This information, along with the sequential nature of the interaction, makes it possible to track topics which are maintained, or dropped, or dropped and then picked up again. It is also possible to examine the extent to which speakers are sharing concepts. These possibilities have been used in analysing various kinds of interactions, including medical consultations (see references below). Discursis also has tools for visualising the analysis, and you can see an example of this below. The data on which these graphics are based is a debate between Kevin Rudd and Tony Abbott held at the National Press Club on 11 August 2013. Figure 1 shows a visualisation of the whole debate. Figure 2 zooms in on a section of the interaction. The boxes on the diagonal represent the speaker turns, and you can see in Figure 2 that hovering the cursor over a box causes the text of that turn to be visible. The boxes back in the matrix represent the conceptual similarity between each pair of turns. A heavily populated column means that the topics in a turn were also in many following turns and a heavily populated row means that a turn shared topics with many preceding turns. Selecting a point of intersection in the matrix displays a similarity score for the turns, and the text of both turns is displayed below the main graphic (not shown here). Discursis was developed by Dan Angus, Janet Wiles and Andrew Smith and has been reworked as an open source tool by staff of Sydney Informatics Hub. A version of the tool running in a Jupyter notebook is available in this Github repository. References ","date":"2022-11-10","objectID":"/posts/discursis/:0:0","tags":null,"title":"Discursis\n","uri":"/posts/discursis/"},{"categories":null,"content":"Acknowledgments This Jupyter notebook and relevant python scripts were developed by the Sydney Informatics Hub (SIH) in collaboration with the Sydney Corpus Lab under the Australian Text Analytics Platform program and the HASS Research Data Commons and Indigenous Research Capability Program. These projects received investment from the Australian Research Data Commons (ARDC), which is funded by the National Collaborative Research Infrastructure Strategy (NCRIS). ","date":"2022-11-10","objectID":"/posts/discursis/:0:1","tags":null,"title":"Discursis\n","uri":"/posts/discursis/"},{"categories":null,"content":"How to cite the notebook: If you are using this notebook in your research, please include the following statement or an appropriate variation thereof: This study has utilised a notebook/notebooks developed for the Australian Text Analytics Platform (https://www.atap.edu.au) available at (https://github.com/Australian-Text-Analytics-Platform/discursis). In addition, please inform ATAP (info@atap.edu.au) of publications and grant applications deriving from the use of any ATAP notebooks in order to support continued funding and development of the platform. ","date":"2022-11-10","objectID":"/posts/discursis/:0:2","tags":null,"title":"Discursis\n","uri":"/posts/discursis/"},{"categories":null,"content":"Launch of ATAP ATAP was officially launched on November 1 2022 at the ResBaz Queensland festival. The event brought together our partners (UQ, University of Sydney and AARNet) and our co-investor (ARDC). You can read more about the launch here, and the video package which was shown will be available on our Vimeo channel soon. Graduate Digital Research Fellowships At the ATAP launch, we also announced a Graduate Digital Research Fellowship program to run in the first part of 2023. If you are a confirmed research student (in SE Queensland) who would like deeper knowledge of how to use digital methods and tools in your research area, GDRF could give you the opportunity to hone your digital skills and to enhance your current research (or to work on an independent digital project). More information ","date":"2022-10-28","objectID":"/contact/:0:0","tags":null,"title":"News","uri":"/contact/"},{"categories":null,"content":"Contact us You can contact the Australian Text Analytics Platform by email We share a Twitter account with the Language Data Commons of Australia: ","date":"2022-10-28","objectID":"/contact/:1:0","tags":null,"title":"News","uri":"/contact/"},{"categories":null,"content":"PDF version This presentation was given By Moises Sacal Bonequi, Ben Foley and Peter Sefton to the HASS RDC and Indigenous Research Capability Program Technical Advisory Group Meeting on 2022-09-02. The Language Data Commons of Australia (LDaCA) and the Australian Text Analytics Platform (ATAP) are establishing a scalable and flexible language data and analytics commons. These projects will be part of the Humanities and Social Sciences Research Data Commons (HASS RDC). The Data Commons will focus on preservation and discovery of distributed multi-modal language data collections under a variety of governance frameworks. This will include access control that reflects ethical constraints and intellectual property rights, including those of Aboriginal and Torres Strait Islander, migrant and Pacific communities. This presentation builds on an overview of the LDaCA and ATAP architecture from 2022-02-11. This time we will zoom in on the Text Analytics side and show progress on linking active workspaces for an analysis with access-controlled data repositories, creating a FAIR-ready platform. Parts of this presentation are recycled from the previous one. For this Research Data Commons work we are using the Arkisto Platform (introduced at eResearch 2020). Arkisto aims to ensure the long term preservation of data independently of code and services, recognizing the ephemeral nature of software and platforms. We know that sustaining software platforms can be hard and aim to make sure that important data assets are not locked up in database or hard-coded logic of some hard-to-maintain application. The above diagram takes a big-picture view of research data management in the context of doing research. It makes a distinction between managed repository storage and the places where work is done - “workspaces”. Workspaces are where researchers collect, analyse and describe data. Examples include the most basic of research IT services, file storage, as well as analytical tools such as Jupyter notebooks (the backbone of ATAP - the text analytics platform). Other examples of workspaces include code repositories such as GitHub or GitLab (a slightly different sense of the word repository), survey tools, electronic (lab) notebooks and bespoke code written for particular research programmes. These workspaces are essential research systems but usually are not set up for long term management of data. The cycle in the centre of this diagram shows an idealised research practice where data are collected and described and deposited into a repository frequently. Data are made findable and accessible as soon as possible and can be “re-collected” for use and re-use. For data to be re-usable by humans and machines (such as ATAP notebook code that consumes datasets in a predictable way) it must be well described. The ATAP and LDaCA approach to this is to use the Research Object Crate (RO-Crate) specification. RO-Crate is essentially a guide to using a number of standards to describe both data and re-runnable software such as workflows or notebooks. This rather messy slide represents the overall high-level architecture for the LDaCA Research Data Commons. There will be an analytical workbench (left of the diagram) which is the basis of the Australian Text Analytics (ATAP) project. This will focus on notebook-style programming using one of the emerging Jupyter notebook platforms in that space. Our engagement lead, Dr Simon Musgrave sees the ATAP work as primarily an educational enterprise encouraging researchers to adopt new research practices, which will be underpinned by services built on the Arkisto standards, allowing for rigorous, re-runnable research. This diagram is a much simpler view zooming in on the core infrastructure components that we have built so far. We are starting with bulk ingest of existing collections and will add one-by-one deposit of individual items after that. This shows the OCFL repository at the bottom, with a Data \u0026 Access API that medi","date":"2022-09-16","objectID":"/posts/atap-arch-preso/:0:0","tags":null,"title":"ATAP Architecture Introduction\n","uri":"/posts/atap-arch-preso/"},{"categories":null,"content":"This work was supported by the Sydney Informatics Hub, a Core Research Facility of the University of Sydney One objective of the Australian Text Analytics Platform is to provide a library of code notebooks that our users – those interested in applying text analytics to research – can learn from and build upon. As the curators of such a resource, we need to learn from what’s been created before for a similar audience; and we need to understand how collections of notebooks can be used as an accessible resource for teaching, critiquing, and creating research using text analytics. These posts therefore summarise the insights we gained from a preliminary survey of key notebook resources. This first part provides an introductory overview of the use of notebooks in text analytics and examines the pedagogically focussed genres of textbooks and tutorials. The second part looks at two other genres, tool chests and research articles, and ends with a summary and a consideration of how the discussion can inform the design of ATAP. ","date":"2022-07-27","objectID":"/posts/notebooks-1/:0:0","tags":["Jupyter","notebooks","text analysis"],"title":"Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 1","uri":"/posts/notebooks-1/"},{"categories":null,"content":"What are notebooks? Notebooks as popularised by the Jupyter project have become a key medium for experimentation and presentation in research and development. They mix code with narrative text, as well as tables, figures and interactive tools generated by the code. Figure 1 shows part of a notebook with some text, a code cell and the results generated by running the code A notebook can be played with interactively, allowing a user to interrogate and tweak the analysis and datasets under construction; but it can also be run as a prefabricated script that fetches a dataset, applies some processing, or generates a report. For researchers, published notebooks provide an opportunity for experimental reproducibility and reusable research pipelines. Notebooks collected together become a library of tutorials, tools, or reproducible research publications. We found several such collections covering applied text analytics, primarily targeting humanities and social sciences (HASS) researchers, as well as some resources that mix code and narrative, but in which the code cannot be executed directly. To make full use of such resources, it is necessary to copy the code and paste it into a console or another environment where it can be run. In addition to having a mix of content, and different approaches to teaching or guiding readers to use text analytics tools, we noted a few broad genres of notebook and types of collection, such as tutorials, tool(kits), apps and research experiments, In this post, we focus on tutorials; the following post will look at the remaining genres.. Before going on, we’d like to note that our survey is by no means complete. We’ve not had time to look in detail at the full diversity of relevant notebook collections, but hope that we have captured useful insights about the state of the art. ","date":"2022-07-27","objectID":"/posts/notebooks-1/:1:0","tags":["Jupyter","notebooks","text analysis"],"title":"Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 1","uri":"/posts/notebooks-1/"},{"categories":null,"content":"Courses for different horses While the various resources all covered core applied text analytics concepts – including acquiring data from various sources, analysing n-gram frequencies, extracting named entities, building and evaluating a topic model, and so on – notebook resources reflected different genres and hence user needs. Some sought to present a rigorous curriculum constructed of tutorials or lessons, others treat collections of notebooks as tools for the user to employ in their work, and others demonstrate the use of a notebook as a single-purpose reproducible research article. ","date":"2022-07-27","objectID":"/posts/notebooks-1/:2:0","tags":["Jupyter","notebooks","text analysis"],"title":"Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 1","uri":"/posts/notebooks-1/"},{"categories":null,"content":"Courses and Textbooks The vast majority of notebook collections we identified positioned themselves as courses, with each notebook corresponding to a tutorial. Some tutorials provide scaffolding for exercises (with or without solutions), but at a baseline, they guide readers/users through the use of a tool or concept. Frequently, the intention is for these tutorial notebooks to be walked through in a workshop, but being openly published as notebooks allows users to execute and play with them outside of the classroom. Tutorials from different authors vary in how much they surround their code with narrative and background explanations of the techniques they are applying. Sinclair and Rockwell’s The Art of Literary Text Analysis (ALTA) provides extensive narrative and appears more like a textbook than a course, not intended for delivery, but for learning and reference. Melanie Walsh’s Introduction to Cultural Analytics \u0026 Python explicitly presents itself as a text book constituted of notebooks, relying on the Jupyterbook tool to give a glossy, accessible presentation, while also supporting the user to interact with the notebooks. Not unlike ALTA, this course demonstrates a deep appreciation of the breadth of technologies available and how they might be applied – and where they might not be applicable or reliable – in digital humanities contexts. It often supports the learner by giving multiple examples of how to apply a technique, which incidentally means repeated opportunities to demonstrate data preparation with diverse inputs. For the R programming community, Martin Schweinberger’s LADAL shows a thoroughness in developing users’ abilities from statistical basics to a breadth of NLP, corpus and computational linguistic methods. (As part of the ATAP project, LADAL tutorials are undergoing conversion from R markdown to Jupyter notebooks.) Technique-oriented introductory tutorials are then complemented with “focus studies” on disciplines of linguistic analysis such as learner language, stylistics or linguistic typology. In comparison to the above sites, several other tutorial resources are much more bare, simply demonstrating how to implement a technique with little use of discussion or repeated examples. The varying levels of narrative, and how they present the code, suggests that authors have a mix of goals that we might query: Is the tutorial introducing a technique, or rather its use through code? Is it teaching the application of a technique, or how it works? Does it aim to teach the reader how to read the code? How to experiment with the code? How to compose that code themselves? Does the notebook itself facilitate those engagements with code, or does the more ephemeral teacher? Although not recently updated, we are indebted to Quinn Dombrowski’s curation of a list of relevant courses and tutorials, among other digital humanities notebooks. A recent player in this space is TAPI (Text Analysis Pedagogy Institute), which places open publication of notebook-based courses at the core of their researcher schooling. TAPI incorporates several courses by individual presenters, although there does not currently appear to be a high level of standardisation among the TAPI courses, including in their licensing. The Programming Historian is also an excellent resource for peer-reviewed digital humanities tutorials, often helpfully grounded in use cases. Not being notebooks (nor R markdown), its tutorials can’t be directly executed, leaving the user to copy-paste its code or not engage interactively with its content. What sets the Programming Historian apart is its design as a journal of tutorials, i.e. a collaborative enterprise in text analytics pedagogy. While resources we surveyed are all essentially open to third party contributors who may offer amendments, most collections of tutorial notebooks have single or few authors and are not designed as primarily collaborative endeavours. Part 2 of this post looks at the remaining genres we h","date":"2022-07-27","objectID":"/posts/notebooks-1/:2:1","tags":["Jupyter","notebooks","text analysis"],"title":"Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 1","uri":"/posts/notebooks-1/"},{"categories":null,"content":"More genres of notebooks and what it all means for ATAP This work was supported by the Sydney Informatics Hub, a Core Research Facility of the University of Sydney Part 1 of this post introduced notebooks in general and identified various broad genres of notebook which have been used in providing text analytic tools. That part looked at notebooks as tutorials and the grouping of such materials into courses or textbooks. In this part, we look at notebooks which introduce individual tools and notebooks which present research results. The post ends with a consideration of how our analysis of different genres of notebook might influence the design of ATAP. ","date":"2022-07-27","objectID":"/posts/notebooks-2/:0:1","tags":["Jupyter","notebooks","text analysis"],"title":"Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 2","uri":"/posts/notebooks-2/"},{"categories":null,"content":"Libraries and tool chests A “course” comes with a sense of order and completeness; a “tutorial” with a sense of guidance and demonstration. Some notebook collections appear more designed for reference or application than teaching, as if each notebook was a tool you could take from a chest and utilise for your needs. A notable collection in this space is Tim Sherratt’s GLAM Workbench. Tim often titles a directory of notebooks as “tools, tips and examples”, which nicely summarises the mix of content that users can retrieve from the Workbench. “Tools” might include a notebook for acquiring Australian parliament’s Hansard transcripts, offering parameters that the user might change for their own project; or an app that summarises query results from the National Library of Australia’s Trove. An app, here, is distinguished by providing interactive widgets within a notebook, rather than expecting the user to set parameters in code and hit “run”. “Examples” includes a case study of filtering a corpus by its metadata, while a walkthrough on language detection applied to Trove incorporates many “tips” which help to identify the author’s thinking as his code takes each turn. Case studies like these lie on a spectrum between the genericity of tutorials often applied to demonstration or toy data, and the hypothesis-driven specificity of a research article. Grounding a topic in research use cases may help to evoke a sense of relevance to the user. Notebooks presented as reusable tools may also be very similar to tutorials. Until recently, Ithaka’s Constellate.org – a core part of TAPI – included tutorial notebooks, and corresponding editions of the same notebook “for research”. (Here’s a tutorial and research pair on finding key terms in a corpus.) Each “research” notebook omits some of the guidance included in the tutorial, and was rather intended to be applied to a user-supplied dataset. The removal of these applied notebooks notwithstanding, it appears that the Constellate tutorials should dually function as reusable tools. The Constellate platform allows the user to create a corpus from JSTOR content, and select a tutorial/tool notebook from their collection, which is then applied to the new corpus. An important usability feature of reusable tools, and tutorials with this capability, is how they guide or support the user to apply the notebook to their own data. Although targeting experienced corporate data scientists, rather than HASS researchers, data infrastructure provider Databricks openly publishes another relevant tool chest of notebooks (albeit not designed for the Jupyter platform). Their Solution Accelerators are designed to support a fairly code-savvy analyst in tackling standard data science problems in industry. (Read “Solution Accelerator” as a less patronising, more businessy, revamp of the term “starter kit”.) A Solution Accelerator notebook mixes narrative and code, inviting the user to bring their own data. It may give them data transformation code, models to apply, questions to ask, and diagnostics to perform. The library of Solution Accelerators shows how even a capable user may appreciate a notebook as a launchpad when working with new techniques. The apparent value of Solution Accelerators to Databricks customers highlights the risk and cost of undertaking analysis without workflow guidance from a notebook. Unlike tutorials, a notebook tool chest does not make great efforts to build up a user’s skills from scratch, but supports them in selecting a tool that might be appropriate, seeing its relevance, and applying it to their needs. ","date":"2022-07-27","objectID":"/posts/notebooks-2/:0:2","tags":["Jupyter","notebooks","text analysis"],"title":"Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 2","uri":"/posts/notebooks-2/"},{"categories":null,"content":"Journals and Anthologies Through their mix of code, narrative and generated tables and figures, notebooks are a powerful tool for presenting reproducible experimental results. A research paper implemented as a notebook may be rebuilt from its code, reproducing tables, figures and examples as the researcher modifies it. An article-as-notebook is distinguished by its focus on a specific research question, and application of whichever tools are appropriate to draw reliable conclusions. Although individual humanities researchers using digital methods might publish their research as code notebooks (see Quinn Dombrowski’s list for instance), institutional or discipline-specific collections of research notebooks are not yet common in applied text analysis and HASS. For comparison, a move towards reproducibility in other sciences allows one to find a multitude of research notebooks in venues like paperswithcode.com and Code Ocean. When driven by a research question, the notebook author is less interested in demonstrating what techniques are available to apply, and more selective in using techniques that may helpfully harness their data towards a specific goal or narrative. The research thesis thus provides a way to specifically evaluate the output of an analysis: does it support a hypothesis? does it induce a new hypothesis? does it suggest problems in the modelling? The final research notebook may hide aspects of this iterative experimentation, evaluation, and the response of further investigation, issue mitigation, or discarding a path of investigation. (The ideal reproducible research notebook would keep track of dead ends too!) Tutorials and tools may emulate these processes of iterative development, or provide a range of possible pathways for analysis; the research notebook, however, must keep its focus on the thesis being explored. The Journal of Digital History is a new player in this space, releasing its first issue in October 2021. This project of the Luxembourg Centre for Contemporary and Digital History and De Gruyter ensures some reproducibility by requiring the code which constructs the research article to be executable and its data available to reproduce tables and figures, while allowing the reader to evaluate and reuse the research implementation. It goes further in harnessing the notebook medium to add layers of depth to peer-reviewed publications in digital history: viewing a JDH publication, a reader is able to hide what they call the “hermeneutic layer” of the work, which provides methodological detail, as distinct from the narrative arc of the research. The notebook medium applied in this way is able to shift several paradigms about how research is constructed, presented, and accessed. ","date":"2022-07-27","objectID":"/posts/notebooks-2/:0:3","tags":["Jupyter","notebooks","text analysis"],"title":"Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 2","uri":"/posts/notebooks-2/"},{"categories":null,"content":"Bringing it back to users In positioning the Australian Text Analytics Platform’s role amidst ongoing and past resource creation, we have to consider how our library of notebooks might build outcomes for our users. Here are some outcomes we are considering. Reference and Application User has access to starters’ kits for a wide range of techniques, covering data modelling, validation and analysis, archiving, etc. User can experiment with standard tools applied to demo data or their own. User can adapt code notebooks to their own use cases. User is able to compare alternative methods for similar goals. Growth and Learning User is familiar with what selected research methods look like as code. User is familiar with what end-to-end research looks like as code. User can prepare their own data for existing or standard tools. User understands how their research problem might translate to a computational problem, and the applicable techniques. User is able to combine multiple techniques to achieve their research goals. User understands how to build good research software. User can confidently build a notebook from scratch for their own research. We also need to define who our user personas are: is it someone with academic expertise, but little in code? or someone with moderate code literacy, but stuck with how to create notebooks for reproducible research? The latter might be more interested in published starter kits, case studies and experiments, while the former needs the guidance provided by a tutorial. ","date":"2022-07-27","objectID":"/posts/notebooks-2/:0:4","tags":["Jupyter","notebooks","text analysis"],"title":"Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 2","uri":"/posts/notebooks-2/"},{"categories":null,"content":"Where to from here? By surveying several collections of notebooks for text analytics, we have a stronger understanding of the spectrum of materials ATAP’s library can incorporate, both in content and in genre. One apparent opportunity is to curate or improve a catalogue of existing resources that will meet the needs of the Australian text analytics research community. It’s also possible to see areas where existing resources may fail to meet the needs of current users of desktop and web tools for corpus analysis from Voyant Tools to AntConc; those tools provide interactivity that carries the user across views of statistics, query matches and texts, but tend to be limited in facilitated reproducible research and in extensibility to arbitrary corpus cleaning and analysis procedures. Tying the resources we’ve looked at to user outcomes also gives a better understanding how to evaluate accessibility of a notebook and the learning opportunities arising from it, such as to what extent a resource: supports the user getting their own data into the notebook. demonstrates the combination of multiple techniques into research. helps a researcher choose techniques relevant to their research question. supports the user to experiment with the code of the notebook demonstrates the processes and challenges around using/applying a technique and the code needed to implement those decisions e.g. decisions to make before and after applying topic models. Overall, we’ve talked little about how notebooks present their content, and how that presentation might affect user engagement and the sustainability of the resource. Of course, those concerns are also important in looking at principles for quality in notebook resources for digital humanities and other text analytics users. ","date":"2022-07-27","objectID":"/posts/notebooks-2/:0:5","tags":["Jupyter","notebooks","text analysis"],"title":"Textbooks, tutorials and tool chests: different ways to learn from notebooks - Part 2","uri":"/posts/notebooks-2/"},{"categories":null,"content":"The ATAP workbench will enable researchers to work in ways which improve the reproducibility and accountability of their research. To do this, the output of your work in ATAP will be a fully documented research object. Research objects are a single package that collects the data, code and other resources used in a piece of research and allows it to be a cite-able research output in its own right. At the end of your work in ATAP, there will be the automated option to package together the components of your workflow, such as: Raw data Transformed data A record of the notebooks which you have used Additional scripts and codes Results Visualisations High quality metadata ATAP will output an RO-Crate that contains these objects. An output of this kind can be assigned a unique identifier (such as a doi), and it can be published via services such as Zenodo or figshare so it can be cited in publications. This will make it easy to fulfil the increasingly common requirement of journals to make available the data that supports a publication. Overview Data Preparation Useful Methods ","date":"2022-02-15","objectID":"/text-analysis/research-objects/:0:0","tags":null,"title":"Research Objects","uri":"/text-analysis/research-objects/"},{"categories":null,"content":"Understanding Text as Data Many definitions of data include an element such as individual items of information. If we consider text to include any sort of language in use, covering different modalities (spoken, written signed) and different extents (from individual sounds to multi-volume books), then fitting text to this definition requires some abstraction. We have to define some unit or units of analysis and we can then treat each of those units as an individual item of information. Examples of such units include documents, sentences and words. But word is not as simple a unit as you may think. ","date":"2022-02-15","objectID":"/text-analysis/overview/:0:1","tags":null,"title":"Text Analysis Overview","uri":"/text-analysis/overview/"},{"categories":null,"content":"What’s in a word? The term word is problematic. It is well-known to linguists that phonological words (defined by sound patterns), syntactic words (defined by combinatorial possibilities) and orthographic words (defined by the conventions of a writing system) do not always coincide. And even when we are only looking at written material, there are problems. How many words are there in this sentence? One answer is that there are six words; that is, there are six groups of characters which are separated according to typographical convention. But there is another answer: There are five words, that is five distinct sequences of characters and one of those sequences (the) occurs twice. The terms standardly used to make this distinction are type and token. Tokens are instances of types, therefore if we count tokens, we count without considering repetition, while if we count types, we do consider repetition. In our example, there are five types (the, cat, sat, on, mat) but six tokens, because there are two tokens of one of the types (the). There is a further distinction we may need to make which we can see if we consider another question: They are distinct types, and therefore must also be distinct as tokens. But we have an intuition that at some level they are related, that there is some more abstract item which underlies both of them. This concept is usually referred to as a lemma (there is more about this concept on the Data Preparation page). ","date":"2022-02-15","objectID":"/text-analysis/overview/:0:2","tags":null,"title":"Text Analysis Overview","uri":"/text-analysis/overview/"},{"categories":null,"content":"Text Analysis Workflow This brief introduction to text analysis divides the process into three parts. In the first stage, the text is made into data. It is divided into the units appropriate for the analysis to be carried out and shaped to a format which our analytic tools can work with. The second stage is the analysis proper, including its interpretation. A wide range of analytic methods can be used, and we give a survey of some of the commonly used possibilities. Finally, our data, methods and results can be documented and packaged as a research object which can be stored and reused. Data Preparation Useful Methods Research Objects ","date":"2022-02-15","objectID":"/text-analysis/overview/:0:3","tags":null,"title":"Text Analysis Overview","uri":"/text-analysis/overview/"},{"categories":null,"content":"Webinars Forthcoming workshops Previous workshops Office Hours ","date":"2022-02-15","objectID":"/events/events/:0:0","tags":null,"title":"Events","uri":"/events/events/"},{"categories":null,"content":"Webinars Our webinar series is a joint initiative with the Language Technology and Data Analysis Laboratory (LADAL), (School of Languages and Cultures, University of Queensland). LADAL sponsored webinars take place in the alternate months. All webinars take place at 8:00PM Brisbane time which is UTC+10. Zoom links will be available one week prior to the event. October 3 2022 - Paweł Kamocki: European Union Data Protection initiatives and their consequences for research Abstract: The European Union, with its large population and GDP, is a leading force in regulatory globalisation. This webinar will discuss recent developments in legal frameworks affecting research data in Europe. Apart from the General Data Protection Regulation which, since its entry into application in 2018, has become an international standard of personal data protection, the recent introduction of statutory copyright exceptions for Text and Data Mining will also be discussed. Moreover, the webinar will also include a presentation of the most recent changes in EU law, such as the Data Governance Act and the Artificial Intelligence Act, which are expected to enter into application in the coming years. Paweł Kamocki is a legal expert in Leibniz-Institut für Deutsche Sprache, Mannheim. He studied linguistics and law, and in 2017 obtained his doctorate in law from the universities of Paris and Münster for a thesis on legal aspects of data-intensive university research, with a focus on Knowledge Commons. He worked as a research and teaching assistant at the Paris Descartes university (now: Université de Paris), then also in the private sector. He is certified to work as an attorney in France. An active member of the CLARIN community since 2012, he currently chairs the CLARIN Legal and Ethical Issues Committee. He also worked with other projects and initiatives in the field of research data policy (RDA, EUDAT) and co-created several LegalTech tools for researchers. One of his main research interests are legal issues in Machine Translation. Zoom link August 1 2022 - Václav Cvrček: The Czech national Corpus Václav Cvrček is a linguist who deals with the description of the Czech language, especially with the use of large electronic corpora and quantitative methods. In 2013-2016 he worked as the director of the Czech National Corpus project, since 2016 he has been the deputy director. Recently, he has been focusing on research on textual variability and corpus-based discourse analysis with a focus on online media. June 6 2022 - Barbara McGillivray: The Journal of Open Humanities Data Barbara McGillivray is a Turing Research Fellow at The Alan Turing Institute, and Editor in Chief of the Journal of Open Humanities Data. Since September 2021 she is also a lecturer in Digital Humanities and Cultural Computation at the Department of Digital Humanities of King’s College London. Before joining the Turing, she was language technologist in the Dictionary division of Oxford University Press and data scientist in the Open Research Group of Springer Nature. Her research at the Turing is on how words change meaning over time and how to model this change in computational ways. She works on machine-learning models for the change in meaning of words in historical times (Ancient Greek, Latin, eighteen-century English) and in contemporary texts (Twitter, web archives, emoji). Her interdisciplinary contribution covers Data Science, Natural Language Processing, Historical Linguistics and other humanistic fields, to push the boundaries of what academic disciplines separately have achieved so far on this topic. 4 April 2022 - Keoni Mahelona: A practical approach to Indigenous data sovereignty Keoni Mahelona is the Chief Technical Officer of Te Hiku Media where he is a part of the team developing the Kaitiakitanga Licence. This licence seeks to balance the importance of publicly accessible data with the reality that indigenous peoples may not have access to the resources that enable them t","date":"2022-02-15","objectID":"/events/events/:0:1","tags":null,"title":"Events","uri":"/events/events/"},{"categories":null,"content":"Forthcoming workshops Workshop on Language Corpora in Australia Over decades of work in Australia, significant collections of language data have been amassed, including of varieties of Australian English, Australian migrant languages, Australian Indigenous languages, sign languages and others. These collections represent a trove of knowledge not only of language in Australia, but also of Australia’s social and cultural history. And yet, not all are well known and many lack published descriptions. The purpose of this workshop is to provide an opportunity to share information about existing language corpora in Australia, with a view to producing a special issue of the Australian Journal of Linguistics that introduces a selection of these corpora, explores how they can contribute to our understanding of language, society, and history in Australia, and considers avenues that such corpora open up for future research. This workshop is being run as part of the Language Data Commons of Australia (LDaCA), which is working to build national research infrastructure for the Humanities and Social Sciences, facilitating access to and use of digital language corpora for linguists, scholars across the Humanities and Social Sciences, and non-academics. Abstract submission For a 20 min presentation, please submit a 250-300 word abstract in English (excluding references). The presentation should include the following information: Speech community/fieldsite: Describe the location of the community and/or their brief history in Australia, the languages spoken and their current status. Corpus design principles: Specify the sample size, sociolinguistic background of the participants, method of data collection and/or genre (e.g. sociolinguistic interviews, natural conversations, oral histories, elicited data, etc.); data format (written/spoken/audio/video, etc.) and where it is stored. Corpus findings and implications: Summarise some key findings from the corpus and discuss other insights that might be obtained from the data in current or future work. Important dates 22 May Abstracts due 5 June Notification of acceptance 3 July Workshop How to Submit: Please submit your abstract by 22 May on https://forms.gle/1pwxVVmUV5hCCZ997 Inquiries Please contact either ","date":"2022-02-15","objectID":"/events/events/:0:2","tags":null,"title":"Events","uri":"/events/events/"},{"categories":null,"content":"Office Hours We invite Australian researchers working with linguistics, text analytics, digital and computational methods, social media and web archives, and much more to attend our regular online office hours, jointly hosted with the Digital Observatory. Bring your technical questions, research problems and rough ideas and get advice and feedback from the combined expertise of our ARDC research infrastructure projects. No question is too small, and even if we don’t know the answer we are likely to be able to point you to someone who does. These sessions run over Zoom from 2-3pm (Australia/Sydney time) every second Tuesday - details. ","date":"2022-02-15","objectID":"/events/events/:0:3","tags":null,"title":"Events","uri":"/events/events/"},{"categories":null,"content":"ATAP is one strand of the partnership between the Australian Research Data Commons (ARDC) and the School of Languages and Cultures at the University of Queensland. This partnership includes a number of projects that explore language-related technologies, data collection infrastructure and Indigenous capability programs. These projects are being led out of the Language Technology and Data Analytics Lab (LADAL), which is overseen by Professor Michael Haugh and Dr Martin Schweinberger. The LDaCA project received investment from ARDC through its Platforms program. Partner Institutions: University of Queensland: Professor Michael Haugh Dr Martin Schweinberger University of Sydney: Professor Monika Bednarek (Sydney Corpus Lab) Sydney Informatics Hub (SIH) AARNet Dr Sara King Ryan Fraser ","date":"2022-02-15","objectID":"/organisation/:0:0","tags":null,"title":"Organisation","uri":"/organisation/"},{"categories":null,"content":"Introducing data preparation concepts This graphic is, sadly, all too true. Data scientists and those who are using data as part of their research spend much of their time preparing their dataset and transforming its structure into a format that can be used (often referred to as data wrangling or data munging). The Australian Text Analytics Platform will offer a range of tools to assist in cleaning text data and performing other preliminary operations which can prepare the data for analysis. ATAP analysis notebooks assume a common data structure, however the platform will provide notebooks containing code for transforming data into the structure that is needed for the procedure(s) in the analysis notebooks. There are two main processes that are needed to prepare text data for analysis , cleaning and annotation, and the ones that you will need to use will depend on the dataset that you are using. ","date":"2022-02-15","objectID":"/text-analysis/data-prep/:0:1","tags":null,"title":"Preparing Text Data","uri":"/text-analysis/data-prep/"},{"categories":null,"content":"Common Cleaning Techniques Making all of the text lower case: This ensures that e.g. dog and Dog will not be treated as different items and is important if you are going to use analytic methods which rely on counting items. However, if you are planning to extract entities from your text data, retaining capital letters may be important. Standardising spelling: At least for English text, there are some well-known spelling variations, some with a geographical context (colour/color) and some that are more a matter of personal preference (recognise/recognize). As with case, standardising spelling ensures that pairs like the examples are treated as tokens of the same type. Removing stopwords: Stopwords are function words that are not interesting for many analyses and we can safely remove them from our data using a stoplist. The 20 most frequently occurring words in the British National Corpus are the, of , and, a, in, to, it, is, was, to, I, for, you, he, be, with, on, that, by , and at. The equivalent list for the Corpus of Contemporary American English (COCA) is the, be, and, of , a, in, to, have, to, it, I, that, for, you, he, with, on, do, say and _ this. Some of the differences between the two are because the COCA counts are of lemmas (see Lemmatization below) which are the base forms of a word (think _ dog and dogs). Packages such as nltk and spaCy include standard stoplists for various languages, and it is possible to specify other words to be excluded. Removing punctuation: Punctuation can change how a text analysis package identifies a word. For instance to be sure that dog and dog? are not treated as different items, removing punctuation is good practice. Removing numbers: Sometimes the presence of numbers in documents can lead to artefacts in analysis. For example, in a collection of documents with page numbering, the numbers might show up as collocates of words or as part of a topic in a topic model. To avoid this, removing numbers is also good practice. This can present a challenge where numbers might be of interest (e.g. a study of mathematics textbooks). Removing whitespace: Whitespace can be another possible source of artefacts in analysis, especially if the source material uses a lot of tabs. ","date":"2022-02-15","objectID":"/text-analysis/data-prep/:0:2","tags":null,"title":"Preparing Text Data","uri":"/text-analysis/data-prep/"},{"categories":null,"content":"Annotation Annotation is the process of adding information to your base dataset in order to make it possible to apply analytic techniques. In some cases, this may be a manual process. For example, much of the annotation which is described in the Text Encoding Initiative Guidelines requires a human making decisions although, in some cases, manual annotation processes may also be scaled up to large text corpora using text classification or information extraction technologies. However some annotation can be carried out automatically, and there are two important kinds of annotation for text which fall into this category. Part-of-speech tagging (POS-tagging): For some analytic procedures, knowing the part of speech (or class of words) that an item belongs to is important. For languages where good pre-trained models exist, this annotation can be carried out automatically to a high level of accuracy – for English, we expect an accuracy rate better than 95%. POS-taggers generally provide more information than just whether an item is a noun or a verb, they also distinguish singular and plural forms of nouns and tell us whether a verb’s form is present tense form or a past tense. The tag sets which are used can therefore be quite extensive, and there are various tag sets in use such as the Penn Treebank tags and the CLAWS tags used by the British National Corpus. LADAL has some excellent resources that discuss POS tagging in more detail. Lemmatization: The distinctions between different forms of a single lexeme can be a hindrance in analysis especially if we are interested in lexical semantics in texts. Lemmatization identifies the base forms of words (or lemmas) in a text so that all forms of an item are treated together. For example: dog and dogs will both be instances of the lemma DOG. eat, eats, eating and ate will all be treated as tokens of the lemma EAT. As noted above, POS-tags give information about the form of words and are generally part of the annotation in lemmatization. A lemma, along with a POS- tag, can be reconstructed to the original form if necessary. Overview Useful methods Research Objects ","date":"2022-02-15","objectID":"/text-analysis/data-prep/:0:3","tags":null,"title":"Preparing Text Data","uri":"/text-analysis/data-prep/"},{"categories":null,"content":"If your university or organisation would like to host a workshop, please contact us. Jefferson Transcript Search Tool The Search Tool project uses programming to explore how to easily search and manipulate transcripts without the need to ‘clean’ the transcript. A browser-based tool has been developed, designed to be used by researchers unfamiliar with programming. The workshop was presented by Evelyn Ansell and is an outcome of her Career Development placement with Australia’s Academic and Research Network (AARNET). The Jupyter Notebook tool and this workshop have been developed during that placement. Date: Friday 17 March 2023 Length: 90 minutes Facilitator: Evelyn Ansell A hands-on guide to Semantic Tagger for your text data analysis The Australian Text Analytics Platform (ATAP) project is a project that aims to provide researchers with the tools and training for analysing, processing, and exploring text. As part of this project, we have adapted with permission, a Semantic Tagger, developed by the University Centre for Computer Corpus Research on Language (UCREL) at Lancaster University. This tool uses the Python Multilingual UCREL Semantic Analysis System (PyMUSAS) to tag your text data so that you can extract token level semantic tags from your text. In addition to the USAS tags, this tool can also recognize Multi Word Expressions (MWE), i.e., expressions formed by two or more words that behave like a unit such as ‘South Australia’, and identifies lemmas and Part-of-Speech (POS) tags in the text. For example, in the sentence ‘President Joe Biden attended two meetings today’, the tool will tag each token with its semantic tag like this -\u003e ‘President Joe Biden’: MWE of [Personal names], ‘attended’: [Participating], ‘two’: [Number], ‘meetings’: [Participating] and ‘today’: [Time: Present; simultaneous]. This tool is available in both English and multi-lingual (Chinese, Italian and Spanish) versions and supports saving the results locally for further analysis, enabling you to gain meaningful insights into your research questions. Date: Wednesday 22 March 2023 Length: 90 minutes Facilitator: Sony Jufri Australian Text Analytics Platform tools: Discursis, Juxtorpus, Quotation tool and Semantic tagger This workshop was part of the USyd Digital Humanities Day 2023. The workshop demonstrated and taught several recently or soon-to-be-released tools from the ATAP text analytic tool collection. These tools include Discursis for analysing human conversational texts, Juxtorpus for advanced corpus slicing and comparison, Semantic Tagger for semantically tagging every word in your text collections, and Quotation Tool for NLP algorithm-based quotation extraction, analysis, and visualisation. Date: Tuesday 14 March 2023 Length: 3 hours Facilitators: Staff of the Sydney Informatics Hub HASS Research Data Commons and IRC Computational Skills Summer School The Australian Research Data Commons (ARDC) through the HASS Research Data Commons and Indigenous Research Capability (HASS RDC and IRC Program) offered a Computational Skills Summer School in Sydney, February 7 and 8, 2023. The Summer School featured skills development workshops to help researchers use the research infrastructure that is being created in the HASS RDC and IRC Program. The projects from the HASS RDC and IRC Program presented workshops on using the tools and platforms. Program Pre-conference workshop (before the 2022 Conference of the Australian Linguistic Society) The Australian Text Analytics Platform and the Language Data Commons of Australia presented a day of workshop activities to give ALS conference delegates (and anyone else who is interested) the opportunity to learn more about the work of the two projects. The day included: an overview of the projects and the work to date reports on specific sub-projects introductory workshops on the tools and resources being developed a workshop on using Discursis, a tool for tracking topics in interactive use of language the opport","date":"2022-02-15","objectID":"/events/previous/:0:0","tags":null,"title":"Previous Workshops","uri":"/events/previous/"},{"categories":null,"content":"Language Technology and Data Analysis Laboratory (LADAL) LADAL aims to help develop computational and digital skills by providing information and practical, hands-on tutorials on data and text analytics as well as on statistical methods relevant for language research. Our Vimeo Channel Recorded presentations and interviews - take a look! GLAM Workbench The GLAM workbench is a collection of tools, tutorials, examples, and hacks created by Tim Sherratt to help you work with data from galleries, libraries, archives, and museums (the GLAM sector). The primary focus is Australia and New Zealand, but new collections are being added all the time. The Sydney Corpus Lab aims to promote corpus linguistics in Australia, hosts a growing list of English-language corpora, and features regular blogs about corpus linguistic analysis. CONSTELLATE Constellate is the text analytics service from the not-for-profit ITHAKA - the same people who brought you JSTOR and Portico. It is a platform for teaching, learning, and performing text analysis using the world’s leading archival repositories of scholarly and primary source content. The Art of Literary Text Analysis The Art of Literary Text Analysis (ALTA) has three objectives. First, to introduce concepts and methodologies for literary text analysis programming. It doesn’t assume you know how to program or how to use digital tools for analyzing texts. Second, to show a range of analytical techniques for the study of texts. While it cannot explain and demonstrate everything, it provides a starting point for humanists with links to other materials. Third, to provide utility notebooks you can use for operating on different texts. These are less well documented and combine ideas from the introductory notebooks. Introduction to Cultural Analytics \u0026 Python is an online textbook by Melanie Walsh, which offers an introduction to the programming language Python that is specifically designed for people interested in the humanities and social sciences. This book demonstrates how Python can be used to study cultural materials such as song lyrics, short stories, newspaper articles, tweets, Reddit posts, and film screenplays. It also introduces computational methods such as web scraping, APIs, topic modeling, Named Entity Recognition (NER), network analysis, and mapping. Text Analysis Pedagogy Institute is an open educational institute for the benefit of teachers (and aspiring teachers) of text analysis in the digital humanities. The Programming Historian publishes novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate research and teaching. Quinn Dombrowski’s list of relevant courses and tutorials A collection of Jupyter notebooks in many human and computer languages for doing digital humanities. ","date":"2022-02-15","objectID":"/resources/:0:0","tags":null,"title":"Resources","uri":"/resources/"},{"categories":null,"content":"ATAP Portal This first version of our user portal provides access to two datasets with notebooks which enable exploration of the data. Farms to Freeways - Data collected in 1991-1992 in a project titled Western Sydney Women’s Oral History Project ‘From farms to freeways: Women’s memories of Western Sydney’, which sought to analyse the experiences of women who had lived in the Blacktown and Penrith areas since the early 1950s, including their responses to social changes brought about by rapid suburbanisation in the Western Sydney region in the post-war period. Corpus of Oz Early English (CoOEE) - Approximately 2 million tokens of material produced in Australia between 1788 and 1900, divided into four time periods and four registers. ","date":"2022-02-15","objectID":"/tools/:0:1","tags":null,"title":"Tools","uri":"/tools/"},{"categories":null,"content":"Discursis Discursis is communication analytics technology that allows a user to analyse text based communication data, such as conversations, web forums and training scenarios. It uses natural language processing algorithms to automatically process transcribed text to highlight participant interactions around specific topics and over the time-course of the conversation. Discursis can assist practitioners in understanding the structure, information content, and inter-speaker relationships that are present within input data. Discursis also provides quantitative measures of key metrics, such as topic introduction; topic consistency; and topic novelty. See this blog post for more details. Discursis was developed by Dan Angus, Janet Wiles and Andrew Smith and has been reworked as an open source tool by staff of Sydney Informatics Hub. ","date":"2022-02-15","objectID":"/tools/:0:2","tags":null,"title":"Tools","uri":"/tools/"},{"categories":null,"content":"Quotation tool This Quotation Tool can be used to extract quotes from a text. In addition to extracting the quotes, the tool also provides information about who the speakers are, the location of the quotes (and the speakers) within the text, the identified named entities, and other information which can be useful for text analysis. See this blog post for more details. ","date":"2022-02-15","objectID":"/tools/:0:3","tags":null,"title":"Tools","uri":"/tools/"},{"categories":null,"content":"Semantic Tagger This Semantic Tagger uses the Python Multilingual Ucrel Semantic Analysis System (PyMUSAS) to tag text so that you can extract token level semantic tags from the tagged text. PyMUSAS, is a rule based token and Multi Word Expression (MWE) semantic tagger. The tagger can support any semantic tagset, however the currently released tagset is for the UCREL Semantic Analysis System (USAS) semantic tags. In addition to the USAS tags, you will also see the lemmas and Part-ofSpeech (POS) tags in the text. For English, the tagger also identifies and tags Multi Word Expressions (MWE), i.e., expressions formed by two or more words that behave like a unit such as ‘South Australia’. ","date":"2022-02-15","objectID":"/tools/:0:4","tags":null,"title":"Tools","uri":"/tools/"},{"categories":null,"content":"Geolocation tools These tools assist the processes of recognising placenames in historical documents and then using online gazetteers to determine what known locations the placenames correspond to, and then to gather related geolocation data such as coordinates. The tools accelerate the workflow for these processes but leave scope for the user to have input in disambiguation. ","date":"2022-02-15","objectID":"/tools/:0:5","tags":null,"title":"Tools","uri":"/tools/"},{"categories":null,"content":"Keyword analysis This notebook presents a Keyword Analysis tool to analyse words in a collection of corpora and identify whether certain words are over or under-represented in a particular corpus compared to their representation in the other corpora. ","date":"2022-02-15","objectID":"/tools/:0:6","tags":null,"title":"Tools","uri":"/tools/"},{"categories":null,"content":"Document Similarity This notebook presents a tool to identify similar documents in your corpus and decide whether to keep them in the corpus or to remove them. See this blog post for more details. ","date":"2022-02-15","objectID":"/tools/:0:7","tags":null,"title":"Tools","uri":"/tools/"},{"categories":null,"content":"Language Technology and Data Analysis Laboratory (LADAL) The Language Technology and Data Analysis Laboratory offers a range of tools for text analysis (and more). ","date":"2022-02-15","objectID":"/tools/:0:8","tags":null,"title":"Tools","uri":"/tools/"},{"categories":null,"content":"Counting words More complex methods - Classification More complex methods - Others Visualisation ","date":"2022-02-15","objectID":"/text-analysis/methods/:0:0","tags":null,"title":"Useful Methods","uri":"/text-analysis/methods/"},{"categories":null,"content":"Introduction Throughout this page, we have given links to further information in Wikipedia and in the tutorials provided by the Language Technology and Data Analysis Laboratory LADAL at the University of Queensland. We also have given references to published research using the methods we discuss. LADAL has an overview of text analysis and distant reading. ","date":"2022-02-15","objectID":"/text-analysis/methods/:0:1","tags":null,"title":"Useful Methods","uri":"/text-analysis/methods/"},{"categories":null,"content":"Counting Words Word frequency Knowing how frequently words occur in a text can already give us information about that text and frequency lists based on large corpora are a useful tool in themselves - you can download such lists for the (original) British National Corpus. Tracking changes in the frequency of use of words across time has become popular since Google’s n-gram viewer has been available. However, results from this tool have to treated with caution for reasons set out in this blog-post. Comparing patterns of word frequency across texts can be part of authorship attribution. Patrick Juola describes using this method when he tried to decide whether Robert Galbraith was really J.K Rowling. This paper uses frequency and concordance analysis, with Australian data: The ratio of types and tokens in a text has been used as a measure of lexical diversity in developmental and clinical studies as well as in stylistics. It has also been applied to theoretical problems in linguistics: Concordance A concordance allows the researcher to see all instances of a word or phrase in a text, neatly aligned in a column and with preceding and following context (see image below). Concordances are often a first step in analysis. The concordance allows a researcher to see how a word is used and in what contexts. Most concordancing tools allow sorting of results by either preceding or following words – the coloured text in the example below shows that in this case the results have been sorted hierarchically on the three following words. This possibility can help in discovering patterns of co-occurrence. Concordances are also very useful when looking for good examples to illustrate a point. (The type of display seen in the example is often referred to as KeyWord In Context – KWIC. There is a possibility of confusion here, as there is a separate analytic method commonly referred to as Keywords.) This tutorial from LADAL on concordancing uses a notebook containing R code as a method of extracting concordance data. Clusters and collocations Two methods can be used for counting the co-occurrence of items in text. Clusters (sometimes known as n-grams) are sequences of adjacent items. A bigram is a sequence of two items, a trigram (3-gram) is a sequence of three items and so on. n-grams are types made up of more than one item, and therefore we can count the number of tokens of each n-gram in texts. n-grams are also the basis for a class of language models. (Google created a very large data set of English n-grams in developing their language-based algorithms and this data is available.) Collocations are patterns of co-occurrence in which the items are not necessarily adjacent. An example of why this is important is verbs and their objects in English. The object of a verb is a noun phrase and in many cases the first item in an English noun phrase is a determiner. This means that for many transitive verbs, the bigram verb the will occur quite frequently. But it is much more interesting to know whether there are patterns relating verbs and the entities which are their objects. Collocation analysis uncovers such patterns by looking at co-occurrences within a window of a certain size, for example three tokens on either side of the target. Collocation analysis gives information about the frequency of the co-occurrence of words and also a statistical measure of how likely that frequency is, given the overall frequencies of the terms in the corpus. Measures commonly applied include Mutual Information scores and Log-Likelihood scores. Collocations can also tell us about the meanings of words. If a word has collocates which fall into semantically distinct groups, this can indicate ambiguity or polysemy. And if different words share patterns of collocation, this can be evidence that the words are at least partial synonyms. This graphic shows collocation relations in Darwin’s Origin of Species visualised as a network - the likelihood of a pair of words occurring i","date":"2022-02-15","objectID":"/text-analysis/methods/:0:2","tags":null,"title":"Useful Methods","uri":"/text-analysis/methods/"},{"categories":null,"content":"More complex methods – Classification Classification methods aim to assign some unit of analysis, such as a word or a document, to a class. For example, a document (or a portion of a document) can be classified as having positive or negative sentiment. These methods are all examples of supervised machine learning. An algorithm is trained on the basis of annotated data to identify classifiers in the data – features which correlate in some way with the annotated classifications. If the algorithm achieves good results on testing data (classified by human judgment), then it can be used to classify unannotated data. Document Classification The task here is to assign documents to categories automatically. An everyday example of this procedure is spam filtering of email that may be applied by internet service providers and also within email applications. An example of this technique being used in research would be automatically identifying historical court records as referring either to violent crimes, property offences, or other crimes. The following two articles taken together give an account of a technique for partially automating the training phase of classification and then of how the classifiers allowed researchers to access new information in a large and complex text. Wikipedia Sentiment analysis Sentiment analysis assigns documents according to the affect which they express. In simple cases, this can mean sorting documents into those which a express a positive view and those which express a negative view (with a neutral position sometimes also included). Such classifications are the basis for aggregated ratings - for example, online listings of movies and restaurants. A sentiment value is assigned to individual reviews then an aggregate score is calculated based on those values and that aggregate score is the rating presented to the user. More sophisticated sentiment analysis can assign values on a scale. Some sentiment analysis tools use dictionaries of terms with sentiment values assigned to those terms; these are known as pre-trained or pre-determined classifiers. The following figure shows the results of the sentiment analysis of four texts (The Adventures of Huckleberry Finn by Mark Twain, 1984 by George Orwell, The Colour out of Space by H.P.Lovecraft, and On the Origin of Species by Charles Darwin) using the Word-Emotion Association Lexicon (Mohammad and Turney 2013). The graphic shows what percentage of each text can be assigned to each of eight categories of sentiment: The Wikipedia entry for Sentiment Analysis gives more information and examples particularly in relation to the use of sentiment analysis as a tool used in online settings. LADAL’s Sentiment Analysis tutorial uses a notebook containing R code as a method of performing sentiment analysis. This article discusses problems in assembling training data for complex sentiment analysis tasks and then applies the results to oral history interviews with Holocaust survivors: Named Entity Recognition Named Entity Recognition involves two levels of classification. First, segments of text are classified as either denoting or not denoting an entity: for example, a person, a place or an organization. The identified entities can then be classified as belonging to one of the types of entity. The Wikipedia entry explaining named-entity recognition gives further detail about the technique. This article looks at the problems encountered when applying a well-known entity recognition package (Stanford) to historical newspapers in the National Library of Australia’s Trove collection: This article (section 6.3) discusses why entity recognition is not as useful as might be expected when studying names in novels: Computational Stylistics (Stylometry) This method is also referred to as authorship attribution as the classification task is to assess patterns of language use in order to decide whether to attribute a piece of text to a particular author (and with what degree of confide","date":"2022-02-15","objectID":"/text-analysis/methods/:0:3","tags":null,"title":"Useful Methods","uri":"/text-analysis/methods/"},{"categories":null,"content":"More complex methods – Others Topic models Topic modeling is a method which tries to recover abstract ‘topics’ which occur in a collection of documents. The underlying assumption is that different topics will tend to be associated with different words, different documents will tend to be associated with different topics, and therefore the distribution of words across documents allows us to find topics. The complete model includes the strength of association (or probability) between each word and each topic, and between each topic and each document. A topic consists of a group of words and it is up to the researcher to decide if a semantically coherent interpretation can be given to any of the topics recovered. The number of topics to be recovered is specified in advance. The example visualisation below is based on topic modeling of the State of the Union addresses given by US presidents, and shows the relative importance of different topics over time. In the right hand part of the figure, the words most closely linked to each topic are listed; the researcher has not attempted to give labels to these (although in some cases, it is not too hard to imagine what labels we might use). Note also that words are not uniquely linked to topics - for example, the word state is closely linked to seven of the topics in this model. The Wikipedia entry for topic models gives a more detailed explanation of the process. This topic modeling tutorial from LADAL uses R coding to process textual data and generate a topic model from that data. Poetics 41(6) is a journal issue devoted to the use of topic models in literary studies: the introduction to the journal (by Mohr and Bogdanov: https://doi.org/10.1016/j.poetic.2013.10.001) provides a useful overview of the method. And this paper uses topic modeling as one tool in trying to improve access to a huge collection of scholarly literature: Network Analysis Network analysis allows us to produce visualisations of the relationships between entities within a dataset. Analysis of social networks is a classic application of the method, but words and documents can also be thought of as entities and the relationships between them can then be analysed with this method. (see example visualisation of Darwin’s Origin of Species above) Here is another example of a network graph illustrating the relationships between the characters of Shakespeare’s Romeo and Juliet: This article gives several examples of how representing collocational links between words as a network can lead to insight into meaning relations: Wikipedia has articles on network theory in general and on social network analysis. in particular. LADAL’s tutorial on Network Analysis introduces this method using R coding. ","date":"2022-02-15","objectID":"/text-analysis/methods/:0:4","tags":null,"title":"Useful Methods","uri":"/text-analysis/methods/"},{"categories":null,"content":"Visualisation Visualisation is an important technique for exploring data, allowing us to see patterns easily, and also for presenting results. There are many methods for creating visualisations and this article gives an overview of some possibilities for visualising corpus data: If you would like to see something more complex, this article includes animations showing change in use of semantic space over time – but you need to have full access to the online publication to see it. This LADAL tutorial on data visualisation in R makes use of the ggplot2 package to create some common data visualisations using code. Overview Data Preparation Research Objects ","date":"2022-02-15","objectID":"/text-analysis/methods/:0:5","tags":null,"title":"Useful Methods","uri":"/text-analysis/methods/"},{"categories":null,"content":"FAIR and CARE Data is becoming increasingly important in today’s world, so corpus linguists might feel that the rest of the world is finally catching up. But the rest of the world are bringing with them new approaches to how data is handled. This means that fields such as corpus linguistics may need to reassess their practices. Such reassessment includes addressing concerns about how data is stored and who can access it (data stewardship) – concerns that are a part of the Open Science movement, ultimately grounded on principles of equity and accountability. The most influential approach to data stewardship today is the FAIR principles. According to these principles, data should be: Findable Metadata and data should be easy to find for both humans and computers. Accessible Once the user finds the required data, she/he/they need to know how can they be accessed, possibly including authentication and authorisation. Interoperable The data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing. Reusable The ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings. In general, corpus linguists do well on the interoperability criterion. Corpus data is usually stored in non-proprietary formats; even when some structure is imposed on the data, this is almost always in a form which is saved as a simple text file (e.g. csv files or xml annotations). Data stored in such formats is easy to move between applications. But what about the other three criteria? Some corpus data is easy to discover; it is findable. For example CLARIN, the portal to the European Union language resource infrastructure, provides access to many large data collections, as does the Linguistic Data Consortium in the USA. However, some data is never made part of a large collection and often remains under the control of individual researchers or research teams. Such data may be almost impossible to find. Even if we can find such data, it is unlikely to be accompanied by good descriptions of the data and metadata, making reusability problematic. Of course, big corpora such as the British National Corpus will be both findable and accompanied by comprehensive corpus manuals. However, it is worth considering how to make other corpora more findable, including the provision of corpus manuals or corpus descriptions. Corpus resource databases such as CoRD do aim to work towards this principle. Accessibility may also be an issue for some data. Copyright law may allow use of material for individual research but prohibit any further distribution of the material. The FAIR approach to such cases is that metadata should be available so that interested parties can know that a data holding exists (F), and the metadata will include information about the conditions under which the data may or may not be shared or reused (A and R). For linguists, there is another very important set of principles concerning data, the CARE principles developed by the Global Indigenous Data Alliance: Collective Benefit Data ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data. Authority to control Indigenous Peoples’ rights and interests in Indigenous data must be recognised and their authority to control such data be empowered. Responsibility Those working with Indigenous data have a responsibility to share how those data are used to support Indigenous Peoples’ self-determination and collective benefit. Ethics Indigenous Peoples’ rights and wellbeing should be the primary concern at all stages of the data life cycle and across the data ecosystem. These principles are presented as applying particularly to Indigenous data, but we believe that researchers should adopt this approach in all cases where the people who participa","date":"2022-02-08","objectID":"/posts/fair-and-care/:0:0","tags":["FAIR","CARE"],"title":"What are the FAIR and CARE principles and why should corpus linguists know about them?","uri":"/posts/fair-and-care/"}]